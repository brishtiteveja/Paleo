
<!-- saved from url=(0106)https://web.archive.org/web/20070711175222/http://luna.cas.usf.edu/~mbrannic/files/regression/Partial.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="./Partial and Semipartial Correlation_files/analytics.js" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app39.us.archive.org';v.server_ms=285;archive_analytics.send_pageview({});});</script><script type="text/javascript" src="./Partial and Semipartial Correlation_files/ait-client-rewrite.js" charset="utf-8"></script>
<script type="text/javascript">
WB_wombat_Init("https://web.archive.org/web/", "20070711175222", "luna.cas.usf.edu:80");
</script>
<script type="text/javascript" src="./Partial and Semipartial Correlation_files/wbhack.js" charset="utf-8"></script>
<script type="text/javascript">
__wbhack.init('https://web.archive.org/web');
</script>
<link rel="stylesheet" type="text/css" href="./Partial and Semipartial Correlation_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./Partial and Semipartial Correlation_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->


<meta name="Generator" content="Microsoft Word 97">
<title>Partial and Semipartial Correlation</title>
<meta name="Template" content="C:\PROGRAM FILES\MICROSOFT OFFICE\OFFICE\html.dot">
</head>
<body link="#0000ff" vlink="#800080" bgcolor="#ccffff"><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<script type="text/javascript" src="./Partial and Semipartial Correlation_files/timestamp.js" charset="utf-8"></script>
<script type="text/javascript" src="./Partial and Semipartial Correlation_files/graph-calc.js" charset="utf-8"></script>
<script type="text/javascript" src="./Partial and Semipartial Correlation_files/auto-complete.js" charset="utf-8"></script>
<script type="text/javascript" src="./Partial and Semipartial Correlation_files/toolbar.js" charset="utf-8"></script>
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
.wb-autocomplete-suggestions {
    text-align: left; cursor: default; border: 1px solid #ccc; border-top: 0; background: #fff; box-shadow: -1px 1px 3px rgba(0,0,0,.1);
    position: absolute; display: none; z-index: 2147483647; max-height: 254px; overflow: hidden; overflow-y: auto; box-sizing: border-box;
}
.wb-autocomplete-suggestion { position: relative; padding: 0 .6em; line-height: 23px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; font-size: 1.02em; color: #333; }
.wb-autocomplete-suggestion b { font-weight: bold; }
.wb-autocomplete-suggestion.selected { background: #f0f0f0; }
</style>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><script type="text/javascript">
__wm.bt(600,27,25,2,"web","http://luna.cas.usf.edu/~mbrannic/files/regression/Partial.html","2007-07-11",1996,"/_static/",['css/banner-styles.css','css/iconochive.css']);
</script><div class="wb-autocomplete-suggestions " style="left: 167px; top: 23px; width: 684px;"></div>
<!-- END WAYBACK TOOLBAR INSERT -->

<b><p align="CENTER">Partial and Semipartial Correlation</p>
</b><p>Give a concrete example (names of variables, context) in which it makes sense to compute a partial correlation. Why a partial rather than a semipartial?</p>
<p>Give a concrete example (names of variables, context) in which it makes sense to compute a semipartial correlation. Why a semipartial rather than a partial?</p>
<p>Why is the squared semipartial always less than or equal to the partial correlation?</p>
<p>Why is regression more closely related to the semipartial than the partial correlation?</p>
<p>Describe how you would go about computing a third order partial correlation.</p>
<b><p align="CENTER">Partial and Semipartial Correlation</p>
</b><p>Regression tends to be a lot more complicated and difficult than ANOVA. The difficulty comes because there are so many concepts in regression and correlation. The excessive number of concepts comes because the problems we tackle are so messy. With ANOVA, you assign people to treatments, and all sorts of explanations of the results (that is, the associations or correlations between the IVs and DV) get ruled out. With nonexperimental data, we cannot assign people to treatments for practical or ethical reasons. People are always interested in the difference between men &amp; women but we really can't assign people to those groups.</p>
<b><p>Partial Correlation</p>
</b><p>We measure individual differences in many things, including cognitive ability, personality, interests &amp; motives, attitudes, and so forth. Many times, we want to know about the influence of one IV on a DV, but one or more other IVs pose an alternative explanation. We would like to hold some third variable constant while examining the relations between X and Y. With assignment we can do this by design. With measures of individual differences, we can do this statistically rather than by manipulation.</p>
<p>The basic idea in partial and semipartial correlation is to examine the correlations among residuals (errors of prediction). If we regress variable X on variable Z, then subtract X' from X, we have a residual e. This e will be uncorrelated with Z, so any correlation X shares with another variable Y cannot be due to Z.</p>
<p>&nbsp;<b>Example</b></p><b>
</b><p>There is at present a debate among educators and policy makers about the use of aptitude and achievement tests as part of college admissions. Some say aptitude tests should be used because they are minimally influenced by formal education. Thus, they tend to level the playing field and account for differences among schools in grade inflation. Other say that achievement tests should be used because they show what people actually know or can do, and they would provide motivation for students to progress beyond basics. There are many complicated arguments that have some merit on both sides. Let's set all that to one side for a moment and think about the utility of such measures for a moment. Suppose what we want to do is to make good admissions decisions in the sense that we want to maximize our prediction of achievement in college from what we know from the end of high school in the area of mathematics. Suppose admit people to college without looking at the data, which are test scores for people on the SAT-Q (quantitative or math aptitude), and scores on a math CLEP test (math achievement) and we look at grades in the standard first year math sequence (differential and integral calculus). We want to know about the prediction of math grades from the two tests.</p>
<p>Our data might look like this:</p>
<table border="" cellspacing="1" bordercolor="#000000" cellpadding="7" width="368">
<tbody><tr><td width="21%" valign="TOP">
<p>&nbsp;Person</p></td>
<td width="26%" valign="TOP">
<p>SAT-Q</p></td>
<td width="26%" valign="TOP">
<p>CLEP</p></td>
<td width="26%" valign="TOP">
<p>Math GPA</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>1</p></td>
<td width="26%" valign="TOP">
<p>500</p></td>
<td width="26%" valign="TOP">
<p>30</p></td>
<td width="26%" valign="TOP">
<p>2.8</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>2</p></td>
<td width="26%" valign="TOP">
<p>550</p></td>
<td width="26%" valign="TOP">
<p>32</p></td>
<td width="26%" valign="TOP">
<p>3.0</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>3</p></td>
<td width="26%" valign="TOP">
<p>450</p></td>
<td width="26%" valign="TOP">
<p>28</p></td>
<td width="26%" valign="TOP">
<p>2.9</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>4</p></td>
<td width="26%" valign="TOP">
<p>400</p></td>
<td width="26%" valign="TOP">
<p>25</p></td>
<td width="26%" valign="TOP">
<p>2.8</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>5</p></td>
<td width="26%" valign="TOP">
<p>600</p></td>
<td width="26%" valign="TOP">
<p>32</p></td>
<td width="26%" valign="TOP">
<p>3.3</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>6</p></td>
<td width="26%" valign="TOP">
<p>650</p></td>
<td width="26%" valign="TOP">
<p>38</p></td>
<td width="26%" valign="TOP">
<p>3.3</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>7</p></td>
<td width="26%" valign="TOP">
<p>700</p></td>
<td width="26%" valign="TOP">
<p>39</p></td>
<td width="26%" valign="TOP">
<p>3.5</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>8</p></td>
<td width="26%" valign="TOP">
<p>550</p></td>
<td width="26%" valign="TOP">
<p>38</p></td>
<td width="26%" valign="TOP">
<p>3.7</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>9</p></td>
<td width="26%" valign="TOP">
<p>650</p></td>
<td width="26%" valign="TOP">
<p>35</p></td>
<td width="26%" valign="TOP">
<p>3.4</p></td>
</tr>
<tr><td width="21%" valign="TOP">
<p>10</p></td>
<td width="26%" valign="TOP">
<p>550</p></td>
<td width="26%" valign="TOP">
<p>31</p></td>
<td width="26%" valign="TOP">
<p>2.9</p></td>
</tr>
</tbody></table>

<p>&nbsp;</p>
<p>The correlations among our three variables are as follows:</p>
<table border="" cellspacing="1" cellpadding="7" width="576">
<tbody><tr><td width="25%" valign="TOP">
<p>&nbsp;</p></td>
<td width="25%" valign="TOP">
<p>SAT-Q</p></td>
<td width="25%" valign="TOP">
<p>CLEP</p></td>
<td width="25%" valign="TOP">
<p>GPA</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>SAT-Q</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>CLEP</p></td>
<td width="25%" valign="TOP">
<p>.87</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>GPA</p></td>
<td width="25%" valign="TOP">
<p>.72</p></td>
<td width="25%" valign="TOP">
<p>.88</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
</tr>
</tbody></table>

<p>&nbsp;</p>
<p>Clearly, both our tests are related to college math mastery as indicated by GPA.</p>
<p>Suppose we regress GPA on SAT-Q. Our regression equation is GPA' = 1.78+.002SATQ and R-square is .52.</p>
<p>If we print our variables, predicted values and residuals, we get:</p>
<table border="" cellspacing="1" bordercolor="#000000" cellpadding="7" width="464">
<tbody><tr><td width="17%" valign="TOP">
<p><font size="1">Person</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">SAT-Q</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">Math GPA</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">Pred</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p align="RIGHT"><font size="1">Resid</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">1</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">500</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">2.8</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.01266</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.21266</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">2</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">550</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.0</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.13544</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.13544</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">3</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">450</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">2.9</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">2.88987</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.01013</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">4</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">400</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">2.8</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">2.76709</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.03291</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">5</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">600</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.3</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.25823</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.04177</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">6</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">650</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.3</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.38101</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.08101</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">7</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">700</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.5</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.50380</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.00380</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">8</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">550</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.7</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.13544</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.56456</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">9</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">650</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">3.4</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.38101</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.01899</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<font size="1"></font><p><font size="1">10</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">550</font></p></td>
<td width="21%" valign="TOP">
<font size="1"></font><p><font size="1">2.9</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">3.13544</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.23544</font></p></td>
</tr>
</tbody></table>

<font size="1"></font><p><font size="1">&nbsp;</font><font face="SAS Monospace" size="1"> </font></p><font face="SAS Monospace" size="1">
</font><p>If we compute the correlations among these variables, we find </p>
<table border="" cellspacing="1" cellpadding="7" width="638">
<tbody><tr><td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">SATQ</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">GPA</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">SATQ</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">GPA</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.72</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1.0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.72</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.69</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
</tr>
</tbody></table>

<font face="SAS Monospace"><p>&nbsp;</p>
</font><p>Note that SAT and GPA are still correlated .72. SAT and PRED are correlated 1.0. After all, PRED is a linear function of SAT (i.e., a linear transformation of the form Y'=1.78+.002SAT). Especially note that RESID is uncorrelated with SATQ, that is, the correlation between PRED and RESID is zero. Of course, the correlation of SAT and RESID is also zero. Remember that the linear model says that the variance in Y is due in part to X and in part to error. The part due to X is a linear function of X that is perfectly correlated with X. What ever is left (the residual) is what is left when the part due to X is subtracted out. Therefore, the residual must be uncorrelated with X. Recall your Venn Diagrams. Just because the residual is uncorrelated with X doesn't mean it cannot correlated with other things. Note that the residual is correlated .69 with GPA. In our case, you might say that the residual is that part of GPA which is left when SAT is taken out. OK, go ahead and say it!</p>
<p>Now we could also do the same thing predicting GPA from math achievement, our CLEP score. If we do that, we find that GPA'=1.17+.06CLEP and R-square =.77. The correlations among these variables are:</p>
<table border="" cellspacing="1" cellpadding="7" width="638">
<tbody><tr><td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">CLEP</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">GPA</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">CLEP</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">GPA</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.88</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1.0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.88</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.48</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
</tr>
</tbody></table>

<p>Note that the correlation between CLEP and GPA is larger than for SAT and GPA. Also note that the correlation between the residual and GPA is smaller. But again the predicted values correlate perfectly with the IV and the residuals do not correlate with the IV or predicted values.</p>
<p>One other thing that we could do help determine a pragmatic argument is to regress GPA on both SAT and CLEP at the same time to see what happens. If we do that, we find that R-square for the model is .78, F = 12.25, p &lt; .01. The intercept and <i>b </i>weight for CLEP are both significant, but the b weight for SAT is not significant. The values are</p>
<p>Intercept = 1.16, t=2.844, p &lt; .05</p>
<p>CLEP = 0.07, t=2.874, p &lt; .05</p>
<p>SATQ = -.0007, t=-0.558, n.s.</p>
<p>In this case, we would conclude that the significant unique predictor is CLEP. Although SAT is highly correlated with GPA, it adds nothing to the prediction equation once the CLEP score is entered. (These data are fictional and the sample size is much too small to run this analysis. It's there for illustration only.)</p>
<p>Now suppose we wanted to argue something a little different. Suppose we had a theory that said that all measures of math achievement share a common explanation, which is math ability. In other words, the reason that various (all) math achievement tests are correlated is that they share the math ability factor. In other words, math ability explains the correlation between achievement tests. In path diagram form, we might represent this something like this:</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><img src="./Partial and Semipartial Correlation_files/p1.gif" width="339" height="254"></p>
<p>Now it may not be immediately obvious, but this diagram says that there is only one common cause of GPA and CLEP, which is SATQ. This implies that the correlation between GPA and CLEP is due solely to SATQ. If there were other theoretical explanations (e.g., motivation), then these should be drawn into the diagram. As it is, this says that the correlation between GPA and CLEP would be zero except for the shared influence of SATQ.</p>
<p>We have already found the residual of GPA when we regressed GPA on SATQ. We know that this residual is not correlated with SATQ. We can run another regression where we predict CLEP from SATQ. If we do this, we find that CLEP' = 8.57+.04SATQ. R-square is .76. We can also see the values of the variables:</p>
<table border="" cellspacing="1" bordercolor="#000000" cellpadding="7" width="464">
<tbody><tr><td width="17%" valign="TOP">
<p>Person</p></td>
<td width="21%" valign="TOP">
<p>SAT-Q</p></td>
<td width="21%" valign="TOP">
<p>CLEP</p></td>
<td width="21%" valign="TOP">
<p>Pred</p></td>
<td width="21%" valign="TOP">
<p align="RIGHT">Resid</p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>1</p></td>
<td width="21%" valign="TOP">
<p>500</p></td>
<td width="21%" valign="TOP">
<p>30</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">30.2025</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-.20253</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>2</p></td>
<td width="21%" valign="TOP">
<p>550</p></td>
<td width="21%" valign="TOP">
<p>32</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">32.3671</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-.36709</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>3</p></td>
<td width="21%" valign="TOP">
<p>450</p></td>
<td width="21%" valign="TOP">
<p>28</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">28.0380</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-.03797</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>4</p></td>
<td width="21%" valign="TOP">
<p>400</p></td>
<td width="21%" valign="TOP">
<p>25</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">25.8734</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-.87342</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>5</p></td>
<td width="21%" valign="TOP">
<p>600</p></td>
<td width="21%" valign="TOP">
<p>32</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">34.5313</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-2.53165</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>6</p></td>
<td width="21%" valign="TOP">
<p>650</p></td>
<td width="21%" valign="TOP">
<p>38</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">36.6952</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">1.30380</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>7</p></td>
<td width="21%" valign="TOP">
<p>700</p></td>
<td width="21%" valign="TOP">
<p>39</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">38.8608</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">0.13924</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>8</p></td>
<td width="21%" valign="TOP">
<p>550</p></td>
<td width="21%" valign="TOP">
<p>38</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">32.3671</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">5.63291</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>9</p></td>
<td width="21%" valign="TOP">
<p>650</p></td>
<td width="21%" valign="TOP">
<p>35</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">36.6962</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-1.69620</font></p></td>
</tr>
<tr><td width="17%" valign="TOP">
<p>10</p></td>
<td width="21%" valign="TOP">
<p>550</p></td>
<td width="21%" valign="TOP">
<p>31</p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">32.3671</font></p></td>
<td width="21%" valign="TOP">
<font face="SAS Monospace"></font><p align="RIGHT"><font face="SAS Monospace">-1.36709</font></p></td>
</tr>
</tbody></table>

<font size="5"></font><p><font size="5">&nbsp;</font> The correlations among these variables are</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<table border="" cellspacing="1" cellpadding="7" width="638">
<tbody><tr><td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">SAT</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">CLEP</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">SAT</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">CLEP</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.87</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">PRED</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1.0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.87</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
<td width="20%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">RESID</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">.49</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">0</font></p></td>
<td width="20%" valign="TOP">
<font face="SAS Monospace"></font><p><font face="SAS Monospace">1</font></p></td>
</tr>
</tbody></table>

<p>Note that the residuals are not correlated with SAT. Now we have two sets of residuals from SAT, one for GPA and one for CLEP. GPA and CLEP are our two achievement measures. According to our theory, they should not be correlated except for the common influence of SAT. The residuals are what is left when we remove SAT from each variable. Therefore, our theory says that our two residuals should not be correlated.</p>
<p>If we compute the correlation between these two sets of residual, we find that:</p>
<table border="" cellspacing="1" cellpadding="7" width="192">
<tbody><tr><td width="50%" valign="TOP">
<p><font face="SAS Monospace" size="1">CLEP resid</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p><font face="SAS Monospace" size="1">GPA resid</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-.2053</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.21266</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-.36709</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.13544</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-.03797</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.01013</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-.87342</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.03291</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-2.53165</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.04177</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">1.30380</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.08101</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.13924</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.00380</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">5.63291</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.56456</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-1.69620</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">0.01899</font></p></td>
</tr>
<tr><td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-1.36709</font></p></td>
<td width="50%" valign="TOP">
<font face="SAS Monospace" size="1"></font><p align="RIGHT"><font face="SAS Monospace" size="1">-0.23544</font></p></td>
</tr>
</tbody></table>

<p>The correlation between the two sets is .73, which is significantly different from zero at p &lt; .05. Thus we can reject our hypothesis that the correlation between GPA and CLEP is explained solely by SAT. Taking it a step further, we may seriously question the theory that the only common cause of the two achievement indices is math ability. Of course, there are always other explanations (our SAT is bad measure of ability? something strange about the sample? the courses that went into the computation of GPA, etc.).</p>
<p>The correlation between the two sets of residuals is called a partial correlation. In our case, it was the correlation between GPA and CLEP while holding SAT constant.</p>
<p>The partial correlation is what we get when we hold constant some third variable from two other variables. We know the correlation between CLEP and GPA is .88. But SAT "accounts for" (or could account for) part of that. What would happen to the correlation if SAT-Q were constant? It is .73, the correlation of the residuals from predicting CLEP and GPA from SATQ. </p>
<p>There are many substantive areas in psychology were we want to know partial correlations (Name 1?).</p>
<p>Pedhazur denotes the partial correlation r<sub>12.3 </sub>where r<sub>12</sub> is the correlation between X<sub>1</sub> and X<sub>2</sub> and the <sub>.3</sub> means the partial controlling for X<sub>3</sub>. In our example, it is the correlation between GPA and CLEP while holding SATQ constant.</p>
<p>The formula to compute the partial <i>r</i> from correlations is </p>
<p><img src="./Partial and Semipartial Correlation_files/p2.gif" width="209" height="75"></p>
<p>In our example, (1 = GPA, 2 = CLEP, 3 = SAT)</p>
<p>&nbsp;</p>
<p><img src="./Partial and Semipartial Correlation_files/p3.gif" width="183" height="45"></p>
<p><img src="./Partial and Semipartial Correlation_files/p3b.gif" width="176" height="40"></p>
<p>You won't be using this equation to figure partials very often, but it's important for two reasons: (1) the partial correlation can be (a little or a lot) larger or smaller then the simple correlation, depending on the signs and size of the correlations used, and (2) for its relation to the semipartial correlation. </p>
<p>If we partial one variable out of a correlation, that partial correlation is called a <i>first order partial correlation</i>. If we partial out 2 variables from that correlation (e.g., r<sub>12.34</sub>), we have a <i>second order partial</i>, and so forth. It is customary to refer to unpartialed (raw, as it were) correlations as <i>zero order correlations</i>. We can use formulas to compute second and higher order partials, or we can use multiple regression to compute residuals. For example, we could regress each of X<sub>1 </sub>and X<sub>2</sub> on both X<sub>3</sub> and X<sub>4 </sub>simultaneously and then compute the correlation between the residuals. </p>
<p>If we did that, we could be computing r<sub>12.34</sub>, the correlation between X<sub>1</sub> and X<sub>2</sub>, controlling for both X<sub>3</sub> and X<sub>4</sub>.</p>
<b><p>Partial Correlations from Multiple Correlations</p>
</b><p>We can compute partials from R<sup>2</sup>. For example</p>
<p><img src="./Partial and Semipartial Correlation_files/p4.gif" width="196" height="86"></p>
<p>Of course we have some confusing terminology for you, but let's explore the meaning of this. This says that the squared first order partial (the partial of 1 and 2 holding 3 constant) is equal to the difference between two R<sup>2</sup> terms divided by 1 minus an R<sup>2</sup> term. The first R<sup>2</sup> term is R<sup>2</sup><sub>1.23</sub>, which is the squared multiple correlation when X<sub>1</sub> is the DV and X<sub>2</sub> and X<sub>3</sub> are the IVs (this is not a partial, it just looks that way to be confusing). The second R<sup>2</sup> is R<sup>2</sup><sub>1.3</sub>, which is the squared correlation when X<sub>1</sub> is the DV and X<sub>3 </sub>is the IV. This is also the term that appears in the denominator. </p>
<p>When we add IVs to a regression equation (first include them), R<sup>2</sup> either stays the same or increases. If the new variable adds to the prediction of the DV, then R<sup>2</sup> increases. If the new variable adds nothing, R<sup>2</sup> stays the same.</p>
<table border="" cellspacing="1" cellpadding="7" width="590">
<tbody><tr><td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p5.gif" width="282" height="211">A</p></td>
<td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p6.gif" width="275" height="206"></p>
<p>B</p></td>
</tr>
<tr><td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p7.gif" width="273" height="203"></p>
<p>C</p></td>
<td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p8.gif" width="275" height="206">D</p></td>
</tr>
</tbody></table>

<p>In Figure A, the R<sup>2</sup> for X<sub>1</sub> will be the overlapping portion Y and X<sub>1</sub> in the figure. When we add X<sub>2</sub> to the equation, R<sup>2</sup> will increase by the part of Y that overlaps with X<sub>2</sub>. Because X<sub>1</sub> and X<sub>2</sub> are orthogonal, R<sup>2</sup> for the model with both X<sub>1</sub> and X<sub>2</sub> will be r<sup>2</sup><sub>y1</sub> + r<sup>2</sup><sub>y2</sub>. In Figure B, when we put X<sub>1</sub> into the regression equation, the R<sup>2</sup> will be the overlapping portion with Y, that is, R<sup>2</sup><sub>y.1</sub> is UY: X<sub>1</sub>+Shared Y. When we add X<sub>2</sub> to the equation, R<sup>2</sup><sub>y.12 </sub>will be the total overlapping portion of Y with both X variables, that is, R<sup>2</sup> will be UY: X<sub>1</sub> + Shared Y + UY: X<sub>2</sub>. The increase in R<sup>2</sup> that we see when we add X<sub>2</sub> if X<sub>1</sub> is already in the equation will be UY: X<sub>2</sub>.</p>
<p>Suppose we start over. We start with X<sub>2</sub> in the regression equation. Then R<sup>2</sup><sub>y.2 </sub>will be UY: X<sub>2</sub> + Shared Y. If we then add X<sub>1</sub> to the equation, R<sup>2</sup> will increase to UY: X<sub>2</sub> + Shared Y + UY: X<sub>1</sub>. In both cases the shared Y is counted only once and it shows up the first time any variable that shares it is included in the model. In Figure C, the variables overlap little, and the addition of each X variable into the equation increases R<sup>2</sup>. In Figure D, X<sub>3</sub> overlaps completely with X<sub>1</sub> and X<sub>2</sub>. If we add X<sub>3 </sub>after X<sub>1</sub> and X<sub>2</sub>, R<sup>2</sup> will not increase. However, adding variables never causes R<sup>2</sup> to decrease (look at the figures).</p>
<p>Now back to the equation:</p>
<p><img src="./Partial and Semipartial Correlation_files/p9.gif" width="208" height="86"></p>
<p>(I've changed symbols slightly to match the figures.) The term on the left is a squared correlation (a shared variance). On the right in the numerator is a difference between two R<sup>2</sup> terms. It is actually an increment in R<sup>2</sup>. It shows the increase in R<sup>2</sup> when we move from predicting Y from X<sub>2</sub> (right term) to predicting Y from X<sub>1</sub> and X<sub>2</sub> (left term). Because R<sup>2</sup> never decreases, R<sup>2</sup><sub>y.12 </sub>will always be greater than or equal to R<sup>2</sup><sub>y.2</sub>. The difference in R<sup>2</sup> will be UY: X<sub>1</sub>, that is, the R<sup>2</sup> due to X<sub>1</sub> above and beyond that due to X<sub>2</sub>. The numerator is the shared variance of Y unique to X<sub>1</sub> (UY: X<sub>1</sub>). So we have partialed out X<sub>2</sub> from X<sub>1</sub> on top. But we still have to remove the influence of X<sub>2</sub> from Y, and this is done in the denominator, where we subtract R<sup>2</sup><sub>Y.2 </sub>from 1. </p>
<p>&nbsp;</p>
<p>The squared correlation is the percentage of shared variance (r<sup>2</sup><sub>Y1.2</sub>). In figure B, the squared partial correlation of X<sub>1</sub> with Y controlling for X<sub>2</sub> will be UY: X<sub>1</sub>/[Total Y-(UY: X<sub>2</sub>+Shared Y)]. Note how X<sub>2</sub> is removed both from X<sub>1</sub> and from Y.</p>
<table border="" cellspacing="1" cellpadding="7" width="666">
<tbody><tr><td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p10.gif" width="311" height="233"></p></td>
<td width="50%" valign="TOP">
<p><img src="./Partial and Semipartial Correlation_files/p11.gif" width="313" height="235"></p></td>
</tr>
</tbody></table>

<p><img src="./Partial and Semipartial Correlation_files/p9.gif" width="208" height="86"></p>
<b><p>Semipartial Correlation</p>
</b><p>With partial correlation, we find the correlation between X and Y holding Z constant for both X and Y. Sometimes, however, we want to hold Z constant for just X or just Y. In that case, we compute a semipartial correlation. A partial correlation is computed between two residuals. A semipartial is computed between one residual and another raw or unresidualized variable. The notation r<sub>1(2.3) </sub>means the semipartial correlation between unmodified X<sub>1</sub> and residualized X<sub>2</sub>, where X<sub>3 </sub>has been taken from X<sub>2</sub>.</p>
<p>Let's compare the correlational formulas for the partial and semipartial--</p>
<p>Partial:</p>
<p><img src="./Partial and Semipartial Correlation_files/p12.gif" width="215" height="75"></p>
<p>Semipartial</p>
<p><img src="./Partial and Semipartial Correlation_files/p13.gif" width="388" height="75"></p>
<p>Note that the partial and semipartial correlation formulas are the same in the numerator and almost the same in the denominator. The partial contains something extra, that is, something missing from the semipartial correlation in the denominator. This means that the partial correlation is going to be larger in absolute value than the semipartial. This will be true except when the controlling or partialling variable is uncorrelated with the variable to be controlled or residualized; this is a trivial case.</p>
<p>Back to our educational debate.  Suppose we want to predict college math grades.  Someone argues that once we know CLEP (advanced achievement in math) scores, there is no need to know SATQ.  SATQ will add nothing to the prediction of GPA once we know CLEP, says the argument.  In this case we will want to partial CLEP from SAT, but not from GPA.  That is, we hold CLEP constant for the SAT, and see whether the SAT so residualized can still predict GPA.</p>
<table border="" cellspacing="1" cellpadding="7" width="384">
<tbody><tr><td width="25%" valign="TOP">&nbsp;</td>
<td width="25%" valign="TOP">
<p>1.  GPA</p></td>
<td width="25%" valign="TOP">
<p>2.  SAT</p></td>
<td width="25%" valign="TOP">
<p>3.  CLEP</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>1.  GPA</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">&nbsp;</td>
<td width="25%" valign="TOP">&nbsp;</td>
</tr>
<tr><td width="25%" valign="TOP">
<p>2.  SAT</p></td>
<td width="25%" valign="TOP">
<p>.72</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">&nbsp;</td>
</tr>
<tr><td width="25%" valign="TOP">
<p>3.  CLEP</p></td>
<td width="25%" valign="TOP">
<p>.87</p></td>
<td width="25%" valign="TOP">
<p>.88</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
</tr>
</tbody></table>

<p>&nbsp;</p>
<p>In our example, (1 = GPA, 2 = SAT, 3 = CLEP)</p>
<p><img src="./Partial and Semipartial Correlation_files/p13a.gif" width="275" height="79"></p>
<p>The correlation between GPA and SAT taking CLEP from SAT is -.096.  This corresponds to the scenario of interest.  It shows that there is basically no correlation between SAT and GPA when we hold CLEP constant.  The other formula for the semipartial shows what happens if we partial CLEP from GPA but not SAT.  This partial is shown below.  It is not really of interest in the current case, but is presented anyway for completeness of computational examples.    </p>
<p>&nbsp;</p>
<p><img src="./Partial and Semipartial Correlation_files/p13b.gif" width="279" height="79"></p>
<p>If we partial the CLEP from both GPA and SAT, the correlation is:</p>
<p>&nbsp;</p>
<p><img src="./Partial and Semipartial Correlation_files/p13b2.gif" width="279" height="79"></p>
<p>The result doesn't make much intuitive sense, but it does remind us that the absolute value of the partial is larger than the semipartial.</p>
<p>One interpretation of the semipartial is that it is the correlation between one variable and the residual of another, so that the influence of a third variable is only paritialed from one of two variables (hence, <i>semipartial</i>). Another interpretation is that the semipartial shows the increment in correlation of one variable above and beyond another. This is seen most easily with the R<sup>2</sup> formulation.</p>
<b><p>Semipartial Correlations from Multiple Correlations</p>
</b><p>Let's compare partial and semipartial <i>squared</i> correlations:</p>
<p>Partial</p>
<p><img src="./Partial and Semipartial Correlation_files/p4.gif" width="196" height="86"></p>
<p>Semipartial</p>
<p><img src="./Partial and Semipartial Correlation_files/p13c.gif" width="168" height="40"></p>
<p>This says that the squared semipartial correlation is equal to the difference between two R<sup>2</sup> values. The difference between the squared partial and semipartial correlations is solely in the denominator. Note that in both formulas, the two R<sup>2</sup> values are incremental. That is, the left R<sup>2</sup> is the squared correlation when X<sub>1</sub> is the DV and X<sub>2</sub> and X<sub>3 </sub>are IVs. The right R<sup>2</sup> is the squared correlation when X<sub>1</sub> is the DV and X<sub>3 </sub>is the IV. The difference between the two values, of course, is due to X<sub>2</sub>. The difference in R<sup>2</sup> is the incremental R<sup>2</sup> for variable X<sub>2</sub>. In terms of our Venn diagrams, X<sub>1</sub> is Y, X<sub>2</sub> is X<sub>1</sub> and X<sub>3 </sub>is X<sub>2</sub>. Therefore, the squared semipartial correlation r<sup>2</sup><sub>y(1.2) </sub>is R<sup>2</sup><sub>y.12</sub> - R<sup>2</sup><sub>y.2</sub> or UY: X<sub>1</sub>. The other semipartial would be R<sup>2</sup><sub>y.12</sub> - R<sup>2</sup><sub>y.1</sub>.</p>
<p>	Both the squared partial and squared semipartial correlations indicate the proportion of shared variance between two variables. The partial tends to be larger than the semipartial. To see why, consider our familiar diagram:</p>
<p><img src="./Partial and Semipartial Correlation_files/p6.gif" width="275" height="206"></p>
<p>The partial correlation of X<sub>1</sub> and Y controlling for X<sub>2</sub> considers the ratio of UY: X<sub>1</sub> to the part of Y that overlaps neither X variable, that is, UY: X<sub>1</sub> to [Y-(Shared Y+UY: X<sub>2</sub>)]. This is because the partial removes X<sub>2</sub> from both X<sub>1</sub> and Y. The semipartial correlation between X<sub>1</sub> and Y r<sub>y(1.2), </sub>however, corresponds the ratio of UY: X<sub>1</sub>to all of Y. This is because X<sub>2</sub> is only taken from X<sub>1</sub>, not from Y.</p>
<p>In our example, </p>
<p>Y = GPA = variable 1</p>
<p>X<sub>1</sub> = CLEP = variable 2; it's r with GPA was .8763, R-square is .7679.</p>
<p>X<sub>2</sub> = SAT = variable 3; its r with GPA was .7181; R-square was .5156.</p>
<p>R-square for GPA on both SAT and CLEP was .7778.</p>
<p><img src="./Partial and Semipartial Correlation_files/p4.gif" width="196" height="86"></p>
<p><img src="./Partial and Semipartial Correlation_files/p13d.gif" width="204" height="54"></p>
<p><img src="./Partial and Semipartial Correlation_files/p3.gif" width="183" height="45"></p>
<p>This agrees with our earlier estimate within rounding error, as .73*.73 = .53.</p>
<p><img src="./Partial and Semipartial Correlation_files/p14.gif" width="168" height="40"></p>
<p><img src="./Partial and Semipartial Correlation_files/p15.gif" width="260" height="40"></p>
<p>Earlier estimate:</p>
<p><img src="./Partial and Semipartial Correlation_files/p16.gif" width="275" height="79"></p>
<p>and .51*.51 = .26.</p>
<b><p>Regression and Semipartial Correlation</p>
</b><p>Regression is about semipartial correlations. For each X variable, we ask "What is the contribution of this X above and beyond the other X variables?" In essence, we regress each new X variable on the other X variables, and then correlate the residualized X with Y. </p>
<i><font size="5"><p align="CENTER">Note that we do <b>NOT</b> residualize Y each time we include an X.</p>
</font></i><p>That would be a partial correlation, not a semipartial correlation. The change in R<sup>2</sup> that we get by including each new X variable in the regression equation is a squared semipartial correlation that corresponds to a <i>b</i> weight. The <i>b</i> weight provides a clue to answering the question "What is the correlation between {X residualized on the other X variables} and {Y}?" Another way of saying this is that the <i>b</i> weight tells us the slope of Y on this X while holding the other X variables in the regression equation constant.</p>
<b><p>Suppressor Variables</p>
</b><p>Suppressor variables are a little hard to understand. I have 3 reasons to discuss them: (1) they prove that inspection of a correlation matrix is not sufficient to tell the value of a variable in a regression equation, (2) sometimes they happen to you, and you have to know what is happening to avoid making a fool of yourself, and (3) they show why Venn diagrams are sometimes inadequate for depicting multiple regression.</p>
<p>The operation of a suppressor is easier to understand if you first think of measured variables as composites (simple or weighted sums) of other variables. </p>
<p>For example, we get a total test score that is the total of the scores on the items of a test. Or we get a job satisfaction overall score that is the total of the facet satisfaction scores. Now suppose that a composite is made by adding two things together that are negatively correlated with one another. For example, suppose we want to know your total attraction to an automobile and we get this by getting your satisfaction with cars by summing your satisfaction with attributes such as price and prestige. So we ask you to rate a bunch of cars on the attributes and we sum them. Now if you like the prestige, you won't like the price, and vice versa. If we add these two things, we get a total satisfaction score, but it has to parts to it that are antagonistic (negatively correlated) across cars. Note that this could happen even if we never asked you for ratings of multiple attributes, but rather asked for your overall satisfaction. Observed measures can be composites of lots of things, some positively correlated, some negatively correlated, and some uncorrelated.</p>
<p>Suppose we have two independent variables; X<sub>1</sub> is correlated with the criterion, and X<sub>2</sub> is not (or nearly so), but it <i>is </i>correlated with the first. Suppose we collected sales performance data (dollars sold per month) for a series of professional sales people (Y). Suppose we ask supervisors for judgment of sales performance for each, that is, how much they like their sales performance (X<sub>1</sub>). We also ask how much each supervisor likes each sales person as a person (X<sub>2</sub>). We have collected some data on these three variables and find that the results can be summarized in the following correlation matrix:</p>
<table border="" cellspacing="1" cellpadding="7" width="590">
<tbody><tr><td width="25%" valign="TOP">
<p>&nbsp;</p></td>
<td width="25%" valign="TOP">
<p>Y</p></td>
<td width="25%" valign="TOP">
<p>X<sub>1</sub></p></td>
<td width="25%" valign="TOP">
<p>X<sub>2</sub></p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>Y</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>X<sub>1</sub></p></td>
<td width="25%" valign="TOP">
<p>.50</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
<td width="25%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="25%" valign="TOP">
<p>X<sub>2</sub></p></td>
<td width="25%" valign="TOP">
<p>.00</p></td>
<td width="25%" valign="TOP">
<p>.50</p></td>
<td width="25%" valign="TOP">
<p>1</p></td>
</tr>
</tbody></table>

<p>&nbsp;</p>
<p>Note that X<sub>1</sub> is correlated with Y. X<sub>2</sub> is not correlated with Y, but it is correlated with X<sub>1</sub>. In this case, X<sub>2</sub> will be a suppressor. We can solve for beta weights by <b>R<sup>-1</sup>r = b</b>.</p>
<table border="" cellspacing="1" cellpadding="7" width="590">
<tbody><tr><td width="13%" valign="TOP">
<p><b>R</b> = </p></td>
<td width="13%" valign="TOP">
<p>1</p></td>
<td width="13%" valign="TOP">
<p>.50</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<b></b><p><b>r</b> =</p></td>
<td width="13%" valign="TOP">
<p>.50</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>.50</p></td>
<td width="13%" valign="TOP">
<p>1</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>.00</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="13%" valign="TOP">
<b></b><p><b>R</b><sup>-1</sup> = </p></td>
<td width="13%" valign="TOP">
<p>1.333</p></td>
<td width="13%" valign="TOP">
<p>-.667</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<b><font face="Symbol"></font></b><p><b><font face="Symbol">b</font> </b>=</p></td>
<td width="13%" valign="TOP">
<p>.667</p></td>
<td width="13%" valign="TOP">
<p>(b1)</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
<tr><td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>-.667</p></td>
<td width="13%" valign="TOP">
<p>1.333</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
<td width="13%" valign="TOP">
<p>-.333</p></td>
<td width="13%" valign="TOP">
<p>(b2)</p></td>
<td width="13%" valign="TOP">
<p>&nbsp;</p></td>
</tr>
</tbody></table>

<p>&nbsp;</p>
<p>Note that the beta weight for X<sub>2</sub> is <b>negative</b> although the correlation between X<sub>2</sub> and Y is zero. This can also happen sometimes when <i>r</i> for X<sub>2</sub> is (usually slightly) positive.</p>
<p>Note also that the beta weight for X<sub>1</sub> is <b>positive</b>, and actually <b>larger </b>than its corresponding <i>r</i> of .50. The R<sup>2</sup> for the two variable model is (.50)*(.667) or .334. This is larger than .50<sup>2</sup> or .25 that would have been guessed solely on the basis of X<sub>1</sub> (X<sub>2</sub> might have been disregarded because of its zero correlation with Y). How can this happen? Three ways to explain the suppressor variable.</p>
<ol>

<li>X<sub>2 </sub>is not correlated with Y, but X<sub>1</sub> is. X<sub>1</sub> is also correlated with X<sub>2</sub>. The part of X<sub>1</sub> that is correlated with X<sub>2</sub> is of no use in predicting Y. If we residualize X<sub>1</sub> on X<sub>2</sub>, X<sub>1</sub> will be better able to predict Y because we will have removed part of it that has nothing to do with Y. X<sub>2 </sub>is a measure of prediction error in X<sub>1</sub>. If we subtract X<sub>2</sub> (the negative <i>b</i> weight) from X<sub>1</sub>, we will improve our prediction (increase the positive <i>b </i>weight for X<sub>1</sub>). Because X<sub>2</sub> is a measure of error, it suppresses the correlation with Y, hence the term suppressor. This is, I think, the reasoning that caused the term.</li>
<li>X<sub>1</sub> and X<sub>2</sub> share something (r = .50) that is not related to Y. When we compute the semipartial correlations of each X with Y, we remove that common part. This makes the b weight increasingly positive for X<sub>1</sub> and negative for X<sub>2</sub>. The reason the X<sub>2</sub> weight is negative is because the part of X<sub>2</sub> that is negatively related to Y is what is left over when we remove the part that is not related to Y; the removed part (taken through residualizing X<sub>2</sub>) was masking the negative relations between X<sub>2</sub> and Y in the raw variables.</li>
<li>The beta weights are taken by finding the inverse of the matrix <b>R</b> and multiplying this by the observed correlations, <b>r</b>. (Although we're talking about beta weights, this applied equally to <i>b</i> weights.) Whenever we have positive correlations among our predictors, the inverse will have to contain negative elements to orthogonalize R. When these negative elements are multiplied by r, the observed correlations, the beta weights will become negative unless the observed correlations are strongly positive. This is a consequence of the positive correlations among the predictors. The suppressor is just a special case of what happens when you invert the predictor matrix in the usual case when independent variables are positively correlated.</li></ol>

<p>Let's return to the three reasons for learning about suppressors. First, <u>inspection of the correlation matrix</u> may be insufficient to tell the value of a variable in a regression equation. It turns out that X<sub>2</sub> was a valuable contributor to predicting Y, and this would not have been obvious from simply looking at the correlations of each X with Y. With just two IVs, you can tell that suppression is likely because of the pattern of correlations. With larger numbers of variables, it becomes increasingly difficult to see what will happen in regression just by looking at <b>R</b>.</p>
<u></u><p><u>Looking like a fool</u>. Always look at your correlations between each X and Y. If the signs of <i>r</i> and <i>b</i> are opposite, you most likely have a suppressor. Do not interpret the negative <i>b </i>weight as if the <i>r</i> were negative. It may be better to interpret the variable with the positive <i>r</i> and negative <i>b</i> as a measure of error of prediction in the set of IVs. You should at least point out to your reader that <i>b</i> and <i>r </i>have opposite signs.</p>
<u></u><p><u>The problem with Venn diagrams</u>. The difficulty here is that in the initial setup, X<sub>2</sub> and Y are not correlated, so the circles do not overlap. After partialing X<sub>1</sub> from X<sub>2</sub>, however, X<sub>2</sub> and Y are negatively correlated, so the circles do overlap. It's hard to draw 1 circle that both does and does not overlap another circle.</p>

<!--
     FILE ARCHIVED ON 17:52:22 Jul 11, 2007 AND RETRIEVED FROM THE
     INTERNET ARCHIVE ON 16:50:58 Sep 09, 2019.
     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.

     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.
     SECTION 108(a)(3)).
-->
<!--
playback timings (ms):
  LoadShardBlock: 105.948 (3)
  CDXLines.iter: 14.535 (3)
  esindex: 0.011
  PetaboxLoader3.datanode: 76.456 (4)
  RedisCDXSource: 0.454
  exclusion.robots.policy: 0.326
  PetaboxLoader3.resolve: 48.111
  exclusion.robots: 0.34
  captures_list: 127.793
  load_resource: 87.563
--><script src="./Partial and Semipartial Correlation_files/bookmarklet.js"></script><script src="./Partial and Semipartial Correlation_files/bookmarklet.js"></script></body></html>